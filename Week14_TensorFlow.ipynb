{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2\n",
    "\n",
    "The second quiz will take place this Wednesday:\n",
    "- 45 minutes\n",
    "- SVM, decision tree, neural network\n",
    "\n",
    "\n",
    "# Final Project Schedule\n",
    "**Proposal **\n",
    "\n",
    "- Form a team of 1-3 students (3-student team is only allowed if the project is significantly more complex than the mid-term project)\n",
    "- Describe the background of the problem,\n",
    "- Describe where to get the data,\n",
    "- Frame the Machine Learning problem: What are input features? What are the model expected to learn? Is it supervised learning / unsupervised learning? Is it a classification / regression problem?\n",
    "- Describe briefly the research plan: what models to use? How to measure the performance?\n",
    "\n",
    "**Each team should submit a project proposal at the beginning of the class on Monday, May 6th.**\n",
    "\n",
    "**Project Submission**\n",
    "Each team is expected provide a Jupyter notebook containing:\n",
    "- Written description on every step and their results. For example, if you decide to build a decision tree model, you should describe how the set up the model (value of maximum depth, maximum number of leaves, etc), and explain how well the model works for the problem.\n",
    "- Executable code that performs the essential steps of the project, including: data cleaning, data visualization/summarization, model training/fine-tuning/evaluation.\n",
    "- A conclusion session that summarizes the project, with explicit statements on the outcome of the project.\n",
    "\n",
    "**Submission Deadline: Wednesday, May 22nd (last day of the exam week)**\n",
    "\n",
    "** Online data sets**\n",
    "- [Kaggle.com](kaggle.com): Gain access to their data set by entering a competition\n",
    "- [UCI machine learning repository](http://mlr.cs.umass.edu/ml/) is one of the oldest sources of data sets on the web. These data sets tend to be fairly small, but are usually clean and ready for machine learning to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "TensorFlow is an open source software library for high performance numerical computation.\n",
    "- Originally it was developed by Google Brain team, and now it is one of the most popular open source projects on GitHub (check out https://github.com/jtoy/awesome-tensorflow)\n",
    "- Its basic principle is simple: first define in Python a graph of computations to perform, and then TensorFlow takes that graph and runs it efficiently using optimized C++ code.\n",
    "![](Data/tf_1.png)\n",
    "- TensorFlow can break up the graph into several chuncks and run them in parallel across multiple CPUs , GPUs, Tensor Processing Units (TPUs), and from desktops to clusters of servers to mobile and edge devices.\n",
    "- It comes with a great visualization tool called TensorBoard that allows you to browse through the computation graph, view learning curves, and more.\n",
    "- Google also launched a cloud service to run TensorFlow computational graphs (cloud.google.com/ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello'\n"
     ]
    }
   ],
   "source": [
    "# Create a constant string\n",
    "hello = tf.constant('hello')  \n",
    "# Create a TensorFlow session\n",
    "sess = tf.Session()\n",
    "# # Print the string during a session run\n",
    "print(sess.run(hello))\n",
    "# print(hello) # this will not work; must execute in a session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataflow Graph\n",
    "TensorFlow uses a **dataflow graph** to represent your computation in terms of the dependencies between individual operations. This leads to a three-step programming procedure:\n",
    "1. Define the dataflow graph\n",
    "2. Create a TensorFlow **session**\n",
    "3. Run the graph\n",
    "\n",
    "A TensorFlow session will take care of placing the operation onto devices such as CPUs and GPUs and running them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Reset a dataflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define two variables x, y\n",
    "x = tf.Variable(3, name='x') # equivalent way in tf for x=3\n",
    "y = tf.Variable(4, name='y') # y=4\n",
    "\n",
    "# Define f based on x and y\n",
    "f = x*x*y + y + 2 \n",
    "\n",
    "# The following print statement only gives the \n",
    "# description of variable f, not its value (since\n",
    "# the value hasn't been computed yet)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow session and evaluates f\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize x and y\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "\n",
    "# Evaluate f\n",
    "result = sess.run(f)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# use with statement to set sess as default session\n",
    "with tf.Session() as sess:\n",
    "    # equivalent to sess.run(x.initializer):\n",
    "    x.initializer.run() \n",
    "    # equivalent to sess.run(y.initializer):\n",
    "    y.initializer.run()\n",
    "    # equivalent to sess.run(f)\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# Use tf.global_variables_initializer() to \n",
    "# initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.56\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: \n",
    "# Create a TensorFlow graph to compute\n",
    "# S = pi * r ** 2\n",
    "# where pi=3.14, r=1.0\n",
    "tf.reset_default_graph()\n",
    "r = tf.Variable(2.0)\n",
    "pi = tf.Variable(3.14)\n",
    "S = pi * r * r\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(S.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding data to the graph\n",
    "We can use tf.placeholder() to delay initialization. This is particularly useful when we want to feed data to the graph during execution. The following code creates a placeholder node A, and B = A + 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8. 9.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, None))\n",
    "# None as a dimension means any size.\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3, 4]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], \n",
    "                                    [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Gradient Descent using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load California housing data\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "housing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.32520000e+00,  4.10000000e+01,  6.98412698e+00,\n",
       "         1.02380952e+00,  3.22000000e+02,  2.55555556e+00,\n",
       "         3.78800000e+01, -1.22230000e+02],\n",
       "       [ 8.30140000e+00,  2.10000000e+01,  6.23813708e+00,\n",
       "         9.71880492e-01,  2.40100000e+03,  2.10984183e+00,\n",
       "         3.78600000e+01, -1.22220000e+02],\n",
       "       [ 7.25740000e+00,  5.20000000e+01,  8.28813559e+00,\n",
       "         1.07344633e+00,  4.96000000e+02,  2.80225989e+00,\n",
       "         3.78500000e+01, -1.22240000e+02],\n",
       "       [ 5.64310000e+00,  5.20000000e+01,  5.81735160e+00,\n",
       "         1.07305936e+00,  5.58000000e+02,  2.54794521e+00,\n",
       "         3.78500000e+01, -1.22250000e+02],\n",
       "       [ 3.84620000e+00,  5.20000000e+01,  6.28185328e+00,\n",
       "         1.08108108e+00,  5.65000000e+02,  2.18146718e+00,\n",
       "         3.78500000e+01, -1.22250000e+02]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing['data'][:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  8.32520000e+00,  4.10000000e+01,\n",
       "         6.98412698e+00,  1.02380952e+00,  3.22000000e+02,\n",
       "         2.55555556e+00,  3.78800000e+01, -1.22230000e+02],\n",
       "       [ 1.00000000e+00,  8.30140000e+00,  2.10000000e+01,\n",
       "         6.23813708e+00,  9.71880492e-01,  2.40100000e+03,\n",
       "         2.10984183e+00,  3.78600000e+01, -1.22220000e+02],\n",
       "       [ 1.00000000e+00,  7.25740000e+00,  5.20000000e+01,\n",
       "         8.28813559e+00,  1.07344633e+00,  4.96000000e+02,\n",
       "         2.80225989e+00,  3.78500000e+01, -1.22240000e+02],\n",
       "       [ 1.00000000e+00,  5.64310000e+00,  5.20000000e+01,\n",
       "         5.81735160e+00,  1.07305936e+00,  5.58000000e+02,\n",
       "         2.54794521e+00,  3.78500000e+01, -1.22250000e+02],\n",
       "       [ 1.00000000e+00,  3.84620000e+00,  5.20000000e+01,\n",
       "         6.28185328e+00,  1.08108108e+00,  5.65000000e+02,\n",
       "         2.18146718e+00,  3.78500000e+01, -1.22250000e+02]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "housing_data_plus_bias[:5, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  2.34476576,  0.98214266,  0.62855945, -0.15375759,\n",
       "        -0.9744286 , -0.04959654,  1.05254828, -1.32783522],\n",
       "       [ 1.        ,  2.33223796, -0.60701891,  0.32704136, -0.26333577,\n",
       "         0.86143887, -0.09251223,  1.04318455, -1.32284391],\n",
       "       [ 1.        ,  1.7826994 ,  1.85618152,  1.15562047, -0.04901636,\n",
       "        -0.82077735, -0.02584253,  1.03850269, -1.33282653],\n",
       "       [ 1.        ,  0.93296751,  1.85618152,  0.15696608, -0.04983292,\n",
       "        -0.76602806, -0.0503293 ,  1.03850269, -1.33781784],\n",
       "       [ 1.        , -0.012881  ,  1.85618152,  0.3447108 , -0.03290586,\n",
       "        -0.75984669, -0.08561576,  1.03850269, -1.33781784]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "scaled_housing_data_plus_bias[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dummy</th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.344766</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.628559</td>\n",
       "      <td>-0.153758</td>\n",
       "      <td>-0.974429</td>\n",
       "      <td>-0.049597</td>\n",
       "      <td>1.052548</td>\n",
       "      <td>-1.327835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.332238</td>\n",
       "      <td>-0.607019</td>\n",
       "      <td>0.327041</td>\n",
       "      <td>-0.263336</td>\n",
       "      <td>0.861439</td>\n",
       "      <td>-0.092512</td>\n",
       "      <td>1.043185</td>\n",
       "      <td>-1.322844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.782699</td>\n",
       "      <td>1.856182</td>\n",
       "      <td>1.155620</td>\n",
       "      <td>-0.049016</td>\n",
       "      <td>-0.820777</td>\n",
       "      <td>-0.025843</td>\n",
       "      <td>1.038503</td>\n",
       "      <td>-1.332827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.932968</td>\n",
       "      <td>1.856182</td>\n",
       "      <td>0.156966</td>\n",
       "      <td>-0.049833</td>\n",
       "      <td>-0.766028</td>\n",
       "      <td>-0.050329</td>\n",
       "      <td>1.038503</td>\n",
       "      <td>-1.337818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.012881</td>\n",
       "      <td>1.856182</td>\n",
       "      <td>0.344711</td>\n",
       "      <td>-0.032906</td>\n",
       "      <td>-0.759847</td>\n",
       "      <td>-0.085616</td>\n",
       "      <td>1.038503</td>\n",
       "      <td>-1.337818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dummy    MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
       "0    1.0  2.344766  0.982143  0.628559  -0.153758   -0.974429 -0.049597   \n",
       "1    1.0  2.332238 -0.607019  0.327041  -0.263336    0.861439 -0.092512   \n",
       "2    1.0  1.782699  1.856182  1.155620  -0.049016   -0.820777 -0.025843   \n",
       "3    1.0  0.932968  1.856182  0.156966  -0.049833   -0.766028 -0.050329   \n",
       "4    1.0 -0.012881  1.856182  0.344711  -0.032906   -0.759847 -0.085616   \n",
       "\n",
       "   Latitude  Longitude  \n",
       "0  1.052548  -1.327835  \n",
       "1  1.043185  -1.322844  \n",
       "2  1.038503  -1.332827  \n",
       "3  1.038503  -1.337818  \n",
       "4  1.038503  -1.337818  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(scaled_housing_data_plus_bias,\n",
    "                  columns=['Dummy'] + housing['feature_names'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Gradient:**\n",
    "- Cost generated from each instance $(\\textbf{x}^{(i)}, y^{(i)})$ is: \n",
    "\n",
    "$(\\theta\\cdot\\textbf{x}^{(i)} - y^{(i)})^2 = (\\theta_0\\cdot 1 + \\theta_1x^{(i)}_1 + \\theta_2x^{(i)}_2 + \\cdots + \\theta_nx^{(i)}_n - y^{(i)})^2$.\n",
    "- Its partial derivative with respect to $\\theta_j$ is:\n",
    "\n",
    "$2x^{(i)}_j(\\theta_0\\cdot 1 + \\theta_1x^{(i)}_1 + \\theta_2x^{(i)}_2 + \\cdots + \\theta_nx^{(i)}_n - y^{(i)})$.\n",
    "- Average cost is: $\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\theta\\cdot\\textbf{x}^{(i)})^2$.\n",
    "- The partial derivative of the average cost with respect to $w_i$ is:\n",
    "\n",
    "$\\frac{1}{m}\\sum_{i=1}^m2x^{(i)}_j(\\theta_0\\cdot 1 + \\theta_1x^{(i)}_1 + \\theta_2x^{(i)}_2 + \\cdots + \\theta_nx^{(i)}_n - y^{(i)})$.\n",
    "\n",
    "**Use tf.gradients(ys, xs) to ask TensorFlow automatically compute the derivatives of sum of ys with respect to xs. **\n",
    "\n",
    "**The update rule of gradient descent:**\n",
    "\n",
    "$\\theta_j = \\theta_j - \\textit{(learning_rate)}\\cdot\\textit{partial derivative}$.\n",
    "\n",
    "The formula is\n",
    "\n",
    "$\\theta_j = \\theta_j - \\textit{learning_rate}\\cdot\\frac{1}{m}\\sum_{i=1}^m2x^{(i)}_j(\\theta_0\\cdot 1 + \\theta_1x^{(i)}_1 + \\theta_2x^{(i)}_2 + \\cdots + \\theta_nx^{(i)}_n - y^{(i)})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.7544265\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.5727804\n",
      "Epoch 300 MSE = 0.5585008\n",
      "Epoch 400 MSE = 0.54907006\n",
      "Epoch 500 MSE = 0.542288\n",
      "Epoch 600 MSE = 0.5373791\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.53124255\n",
      "Epoch 900 MSE = 0.5293705\n",
      "Epoch 1000 MSE = 0.52801067\n",
      "Epoch 1100 MSE = 0.52702194\n",
      "Epoch 1200 MSE = 0.5263023\n",
      "Epoch 1300 MSE = 0.52577746\n",
      "Epoch 1400 MSE = 0.52539444\n",
      "Epoch 1500 MSE = 0.5251144\n",
      "Epoch 1600 MSE = 0.52490914\n",
      "Epoch 1700 MSE = 0.5247584\n",
      "Epoch 1800 MSE = 0.5246476\n",
      "Epoch 1900 MSE = 0.52456564\n",
      "Epoch 2000 MSE = 0.5245052\n",
      "Epoch 2100 MSE = 0.5244602\n",
      "Epoch 2200 MSE = 0.5244267\n",
      "Epoch 2300 MSE = 0.52440166\n",
      "Epoch 2400 MSE = 0.5243829\n",
      "Epoch 2500 MSE = 0.52436876\n",
      "Epoch 2600 MSE = 0.524358\n",
      "Epoch 2700 MSE = 0.52434987\n",
      "Epoch 2800 MSE = 0.5243435\n",
      "Epoch 2900 MSE = 0.52433884\n",
      "Epoch 3000 MSE = 0.5243351\n",
      "Epoch 3100 MSE = 0.5243322\n",
      "Epoch 3200 MSE = 0.52432984\n",
      "Epoch 3300 MSE = 0.5243281\n",
      "Epoch 3400 MSE = 0.5243267\n",
      "Epoch 3500 MSE = 0.5243256\n",
      "Epoch 3600 MSE = 0.5243248\n",
      "Epoch 3700 MSE = 0.52432406\n",
      "Epoch 3800 MSE = 0.5243235\n",
      "Epoch 3900 MSE = 0.5243231\n",
      "Epoch 4000 MSE = 0.52432275\n",
      "Epoch 4100 MSE = 0.5243224\n",
      "Epoch 4200 MSE = 0.52432215\n",
      "Epoch 4300 MSE = 0.5243219\n",
      "Epoch 4400 MSE = 0.52432173\n",
      "Epoch 4500 MSE = 0.5243216\n",
      "Epoch 4600 MSE = 0.5243215\n",
      "Epoch 4700 MSE = 0.52432144\n",
      "Epoch 4800 MSE = 0.5243213\n",
      "Epoch 4900 MSE = 0.52432126\n",
      "Best theta:\n",
      "[[ 2.0685525 ]\n",
      " [ 0.8290128 ]\n",
      " [ 0.11867213]\n",
      " [-0.2643179 ]\n",
      " [ 0.3046684 ]\n",
      " [-0.00452402]\n",
      " [-0.03930663]\n",
      " [-0.9010342 ]\n",
      " [-0.8716156 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 5000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the previous result with linear regression from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(scaled_housing_data_plus_bias,\n",
    "          housing.target.reshape([-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.8296193   0.11875165 -0.26552688  0.30569623 -0.004503\n",
      "  -0.03932627 -0.89988565 -0.870541  ]] [2.06855817]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Neural Network using plain TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/data\\train-images-idx3-ubyte.gz\n",
      "Extracting tmp/data\\train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('tmp/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype('int')\n",
    "y_test = mnist.test.labels.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32,\n",
    "                   shape=(None, n_inputs),\n",
    "                   name='x')\n",
    "y = tf.placeholder(tf.int64,\n",
    "                   shape=(None),\n",
    "                   name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal(\n",
    "            (n_inputs, n_neurons),\n",
    "            stddev=stddev\n",
    "        )\n",
    "        W = tf.Variable(init, name='weights')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]),\n",
    "                        name='bias')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        return activation(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = neuron_layer(\n",
    "        X,\n",
    "        n_hidden1,\n",
    "        name='hidden1',\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "    hidden2 = neuron_layer(\n",
    "        hidden1,\n",
    "        n_hidden2,\n",
    "        name='hidden2',\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "    logits = neuron_layer(\n",
    "        hidden2,\n",
    "        n_outputs,\n",
    "        name='outputs',\n",
    "        activation=tf.identity\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y,\n",
    "        logits=logits\n",
    "    )\n",
    "    loss = tf.reduce_mean(xentropy,\n",
    "                          name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(\n",
    "        learning_rate\n",
    "    )\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct, tf.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Accuracy: 0.9176\n",
      "1 Accuracy: 0.9319\n",
      "2 Accuracy: 0.9412\n",
      "3 Accuracy: 0.9469\n",
      "4 Accuracy: 0.9509\n",
      "5 Accuracy: 0.9554\n",
      "6 Accuracy: 0.9566\n",
      "7 Accuracy: 0.9597\n",
      "8 Accuracy: 0.9599\n",
      "9 Accuracy: 0.9629\n",
      "10 Accuracy: 0.9644\n",
      "11 Accuracy: 0.9663\n",
      "12 Accuracy: 0.9676\n",
      "13 Accuracy: 0.9686\n",
      "14 Accuracy: 0.9688\n",
      "15 Accuracy: 0.9701\n",
      "16 Accuracy: 0.9704\n",
      "17 Accuracy: 0.9696\n",
      "18 Accuracy: 0.9721\n",
      "19 Accuracy: 0.9723\n",
      "20 Accuracy: 0.973\n",
      "21 Accuracy: 0.9739\n",
      "22 Accuracy: 0.974\n",
      "23 Accuracy: 0.974\n",
      "24 Accuracy: 0.9758\n",
      "25 Accuracy: 0.9751\n",
      "26 Accuracy: 0.9752\n",
      "27 Accuracy: 0.9759\n",
      "28 Accuracy: 0.9757\n",
      "29 Accuracy: 0.9758\n",
      "30 Accuracy: 0.9758\n",
      "31 Accuracy: 0.9771\n",
      "32 Accuracy: 0.9762\n",
      "33 Accuracy: 0.9768\n",
      "34 Accuracy: 0.9763\n",
      "35 Accuracy: 0.9771\n",
      "36 Accuracy: 0.9775\n",
      "37 Accuracy: 0.9771\n",
      "38 Accuracy: 0.9776\n",
      "39 Accuracy: 0.9782\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op,\n",
    "                     feed_dict={X: X_batch,\n",
    "                                y: y_batch})\n",
    "        acc = accuracy.eval(\n",
    "            feed_dict={X: X_test,\n",
    "                       y: y_test}\n",
    "        )\n",
    "        print(epoch, 'Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
