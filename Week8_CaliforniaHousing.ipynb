{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: An End-to-End Machine Learning Project\n",
    "\n",
    "In this notebook, we will go through an end-to-end example project, pretending to be a recently hired data scientist in a real estate company. Here are the main steps we will go through:\n",
    "\n",
    "1. Look at the big picture.\n",
    "2. Get the data.\n",
    "3. Discover and visualize the data to gain insights.\n",
    "4. Prepare the data for Machine Learning algorithms.\n",
    "5. Select a model and train it.\n",
    "6. Fine-tune the model.\n",
    "7. Present the solution.\n",
    "8. Launch, monitor, and maintain the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Look at the big picture\n",
    "\n",
    "For this project, we are asked to build a model of housing prices in California using the California census data. This data has metrics such as the population, median income, median housing price, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Buearu publishes sample data (a block group typically has a population of 600 to 3000 people). The model should learn from this data and be able to predict the median housing price in any district, given all the other metrics. This model will be used as a factor to determine whether it is worth investing in a given area or not.\n",
    "\n",
    "**Frame the problem**\n",
    "\n",
    "1. Define the project objective.\n",
    "2. How will the solution be used?\n",
    "3. What are the current solutions?\n",
    "4. What type of Machine Learning is needed?\n",
    "5. How should performance be measured?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the data\n",
    "\n",
    "In typical environments your data would be available in a relational database and spread across multiple tables/documents/files. To access it, you would first need to get your credentials and access authorizations. \n",
    "\n",
    "In this project, the data can be simply downloaded from \n",
    "\n",
    "\"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz\"\n",
    "\n",
    "Extract *housing.csv* from the tgz file, then load the file as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "housing = pd.read_csv(os.getcwd() + '/Data/CaliforniaHousing/housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore the data\n",
    "\n",
    "1. Display the top five rows using the head() method. Learn the attributes in the data.\n",
    "2. Get a quick description of the data using the info() method. Learn the total number of rows, each attribute's type, and number of non-null values.\n",
    "3. Display the frequencies of categorial attributes using the value_counts() method.\n",
    "4. Display a summary of numerical attributes using the describe() method.\n",
    "5. Plot historgrams for each numerical attribute to get a feel of its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.ocean_proximity.value_counts()\n",
    "# housing['ocean_proximity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed these things?\n",
    "1. There are 207 missing values for attribute total_bedrooms.\n",
    "1. The median income attribute does not look like it is expressed in US dollars.\n",
    "2. The housing median age and the median house value are capped.\n",
    "3. These attributes have very different scales.\n",
    "4. Most attributes are right-skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Geographical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. scatter plot of geographical data\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. scatter plot with colors indicating house values\n",
    "housing.plot(kind=\"scatter\",\n",
    "             x=\"longitude\",\n",
    "             y=\"latitude\",\n",
    "             alpha=0.4,\n",
    "             s=housing[\"population\"]/100,\n",
    "             label=\"population\",\n",
    "             figsize=(10,7),\n",
    "             c=\"median_house_value\",\n",
    "             cmap=plt.get_cmap(\"jet\"),\n",
    "             colorbar=True,\n",
    "             sharex=False)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell requires california.png file downloaded\n",
    "# from textbook GitHub repository\n",
    "\n",
    "# scatter plot on California map\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "california_img=mpimg.imread('Data/CaliforniaHousing/california.png')\n",
    "ax = housing.plot(kind=\"scatter\",\n",
    "                  x=\"longitude\",\n",
    "                  y=\"latitude\",\n",
    "                  figsize=(10,7),\n",
    "                  s=housing['population']/100,\n",
    "                  label=\"Population\",\n",
    "                  c=\"median_house_value\",\n",
    "                  cmap=plt.get_cmap(\"jet\"),\n",
    "                  colorbar=False,\n",
    "                  alpha=0.4)\n",
    "plt.imshow(california_img,\n",
    "           extent=[-124.55, -113.80, 32.45, 42.05],\n",
    "           alpha=0.5)\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) \\\n",
    "                         for v in tick_values], fontsize=14)\n",
    "cbar.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between attributes\n",
    "\n",
    "1. Use .corr() to display the standard correlation coefficient between median house value and each input feature.\n",
    "2. Use pandas.plotting.scatter_matrix() to visualize the correlation.\n",
    "\n",
    "- Correlation coefficient ranges from -1 to 1.\n",
    "- When it is close to 1, it means that there is a strong positive correlation.\n",
    "- When it is close to -1, it means that there is a strong negative correlation.\n",
    "- When it is close to zero, it means that there is no *linear* correlation (other correlation may still exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.corr()['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with combinations of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n",
    "             alpha=0.2)\n",
    "plt.axis([0, 5, 0, 520000])\n",
    "plt.show()\n",
    "print(housing.corr()['median_house_value'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the data for Machine Learning algorithms\n",
    "\n",
    "\n",
    "1. Impute missing values with median (use sklearn.preprocessing.Imputer).\n",
    "2. Convert categorical attributes to numerical attributes (use sklearn.preprocessing.OneHotEncoder).\n",
    "3. Add extra useful attributes (customized transformer)\n",
    "4. Feature scaling: use sklearn.preprocessing.StandardScaler to scale attributes to zero mean and unit variation.\n",
    "5. Split the data into training set and test set. (Should we use purely randomized splitting?)\n",
    "\n",
    "**Question**\n",
    "- Should we use simple random sampling to obtain test data?\n",
    "- Should we use the entire dataset to build feature scaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute total_bedrooms with median value\n",
    "# median = housing['total_bedrooms'].median()\n",
    "# housing['total_bedrooms'].fillna(median, inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "housing_num_columns = housing_num.columns\n",
    "housing_cat = housing['ocean_proximity']\n",
    "imputer = Imputer(strategy='median')\n",
    "imputer.fit(housing_num)\n",
    "print('median values:', housing_num.median().values)\n",
    "print('imputer statistics:', imputer.statistics_)\n",
    "housing_num = imputer.transform(housing_num)\n",
    "housing_num = pd.DataFrame(housing_num,\n",
    "                           columns=housing_num_columns)\n",
    "housing_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess categorical feature 'ocean_proximity'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
    "print('housing_categories:', housing_categories)\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))\n",
    "print(housing_cat_1hot.toarray()[:5])\n",
    "\n",
    "housing_cat = pd.DataFrame(housing_cat_1hot.toarray(),\n",
    "                           columns=housing_categories)\n",
    "housing_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom transformer to add extra attributes: \n",
    "# 1. rooms_per_household\n",
    "# 2. population_per_household\n",
    "# 3. (optional) bedrooms_per_room\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# column index\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, bedrooms_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X,\n",
    "                         rooms_per_household,\n",
    "                         population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X,\n",
    "                         rooms_per_household,\n",
    "                         population_per_household]\n",
    "\n",
    "# apply the above class to add extra attributes\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing_num.values)\n",
    "\n",
    "# convert it to a dataframe\n",
    "housing_extra_attribs = pd.DataFrame(housing_extra_attribs,\n",
    "                                     columns=list(housing_num.columns)+['rooms_per_household',\n",
    "                                                                    'population_per_household',\n",
    "                                                                    ])\n",
    "housing_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "housing_prepared = pd.concat([housing_extra_attribs, housing_cat], axis=1)\n",
    "housing_prepared_columns = housing_prepared.columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "housing_prepared = scaler.fit_transform(housing_prepared)\n",
    "housing_prepared = pd.DataFrame(housing_prepared,\n",
    "                            columns=housing_prepared_columns)\n",
    "print('shape:', housing_prepared.shape)\n",
    "housing_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "housing_train, housing_test = train_test_split(housing_prepared,\n",
    "                                               test_size=0.2,\n",
    "                                               random_state=1)\n",
    "print('training set:', housing_train.shape)\n",
    "print('test set:', housing_test.shape)\n",
    "housing_train_labels = housing_train.pop('median_house_value')\n",
    "housing_test_labels = housing_test.pop('median_house_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Select and Train a Model\n",
    "\n",
    "1. Apply a Machine Learning model to the training set.\n",
    "2. Measure the performance of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_train, housing_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_train_predictions = lin_reg.predict(housing_train)\n",
    "lin_mse = mean_squared_error(housing_train_labels,\n",
    "                             housing_train_predictions)\n",
    "print('MSE on training set', lin_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lin_mae = mean_absolute_error(housing_train_labels,\n",
    "                              housing_train_predictions)\n",
    "print('MAE on training set', lin_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_train, housing_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_predictions = tree_reg.predict(housing_train)\n",
    "tree_mse = mean_squared_error(housing_train_labels,\n",
    "                              housing_train_predictions)\n",
    "print('MSE on training set', tree_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(housing_train, housing_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_predictions = forest_reg.predict(housing_train)\n",
    "forest_mse = mean_squared_error(housing_train_labels,\n",
    "                              housing_train_predictions)\n",
    "print('MSE on training set', forest_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better evaluation using cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg,\n",
    "                         housing_train,\n",
    "                         housing_train_labels,\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=10)\n",
    "print('Decision Tree:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lin_reg,\n",
    "                         housing_train,\n",
    "                         housing_train_labels,\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=10)\n",
    "print('Linear Regression:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(forest_reg,\n",
    "                         housing_train,\n",
    "                         housing_train_labels,\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=10)\n",
    "print('Random Forest:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fine-Tune the Model\n",
    "Search for a good combination of hyperparameter values for random forest model.\n",
    "- Grid search: give a few possible values for each hyperparameter, then try all combinations.\n",
    "- Random search: select values randomly (efficient when there are a large number of hyperparameters)\n",
    "\n",
    "**Analyze the best model and its error**\n",
    "- Does this model make sense?\n",
    "- Should less important features be dropped?\n",
    "- Does the model make any typical errors?\n",
    "\n",
    "**Evaluate the model on test set**\n",
    "- (transform the test data)\n",
    "- analyze the performance of the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30],\n",
    "     'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap\n",
    "    # set as False\n",
    "    {'bootstrap': [False],\n",
    "     'n_estimators': [3, 10],\n",
    "     'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of\n",
    "# (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(\\\n",
    "                   forest_reg,\n",
    "                   param_grid,\n",
    "                   cv=5,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   return_train_score=True,\n",
    "                          )\n",
    "grid_search.fit(housing_train,\n",
    "                housing_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best hyperparameter combination found:\n",
    "print('best parameters:', grid_search.best_params_)\n",
    "\n",
    "# The best model with above parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "housing_train_pred = best_model.predict(housing_train)\n",
    "print('MSE:', mean_squared_error(housing_train_pred,\n",
    "                                 housing_train_labels))\n",
    "scores = cross_val_score(best_model,\n",
    "                          housing_train,\n",
    "                          housing_train_labels,\n",
    "                          cv=10,\n",
    "                          scoring=\"neg_mean_squared_error\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized search\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {'bootstrap': [True, False],\n",
    "          'n_estimators': randint(2, 30),\n",
    "          'max_features': randint(2, 10),}\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\\\n",
    "                   forest_reg,\n",
    "                   params,\n",
    "                   cv=5,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   return_train_score=True,\n",
    "                                  )\n",
    "random_search.fit(housing_train,\n",
    "                  housing_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The best hyperparameter combination found:\n",
    "print('best parameters:', random_search.best_params_)\n",
    "\n",
    "# The best model with above parameters\n",
    "best_model = random_search.best_estimator_\n",
    "housing_train_pred = best_model.predict(housing_train)\n",
    "print('MSE:', mean_squared_error(housing_train_pred,\n",
    "                                 housing_train_labels))\n",
    "scores = cross_val_score(best_model,\n",
    "                          housing_train,\n",
    "                          housing_train_labels,\n",
    "                          cv=10,\n",
    "                          scoring=\"neg_mean_squared_error\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the best model\n",
    "feature_importance = best_model.feature_importances_\n",
    "attributes = housing_train.columns\n",
    "sorted(zip(feature_importance, attributes),\n",
    "       reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation the model on test set\n",
    "housing_test_pred = best_model.predict(housing_test)\n",
    "best_mse = mean_squared_error(housing_test_labels,\n",
    "                              housing_test_pred)\n",
    "print('MSE on test set:', best_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch, Monitor, and Maintain the model\n",
    "- Monitor the live performance of the system\n",
    "- Retrain the model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
